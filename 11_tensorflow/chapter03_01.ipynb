{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(401.05988, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_sum(tf.random.normal([1000,1000])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow 2.10.0 시작하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "# Hello world \n",
    "# 텐서플로는  C++와 python을 기반으로 한다. Google Colab도 python을 기본적으로 지원하고 있다.\n",
    "print(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 텐서플로는 GitHub에 공개된 open source library로서 약 75,000개의 fork가 만들어지고 2,100여명의 코드 기여자가 있으며, 거의 매 주마다 마이너 버전이 업데이트된다. Library의 version정보가 다르면 잘 돌아가야 할 코드에서 에러가 발생할 수도 있다. 설치한 텐서플로의 버전을 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### perceptron의 한계를 지적하는데 사용됐던 AND, OR, XOR 연산을 할 수 있는 신경망 네트워크를 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.17867386], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# [0,1] 구간의 uniform distribution에서 shape=(1,)인 난수 생성\n",
    "rand = tf.random.uniform([1], 0, 1)\n",
    "print(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.20703745 0.8988174  0.37414896 0.82971346], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# [0,1] 구간의 uniform distribution에서 shape=(4,)인 난수 생성\n",
    "rand = tf.random.uniform(shape=[4], minval=0, maxval=1)\n",
    "print(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.81094915  0.4870714 ]\n",
      " [-1.6467528  -1.3862404 ]\n",
      " [-1.4208333  -2.8433623 ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 평균이 0, 표준편차가 1 인 정규분포에서 shape=(3,2)인 난수 행렬 생성\n",
    "rand = tf.random.normal(shape=[3,2], mean=0, stddev=1)\n",
    "print(rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 신경망의 가장 기본적인 구성요소인 뉴런을 만들어 보자.<br>뉴런이 여러개 모여 layer를 구성한 후 이 layer들이 모여서 신경망을 구성한다.<br>뉴런은 입력, 가중치, 활성화함수, 출력으로 구성된다.<br>가장 간단한 형태의 뉴런은 입력에 가중치를 곱한 뒤(행렬의 곱셈) 활성화함수를 취하면 출력을 얻을 수 있다.<br>뉴런에서 학습할 때 변하는 것은 가중치이다. 가중치는 처음에는 초기화를 통해 랜덤한 값을 넣고, 학습 과정에서 점차 일정한 값으로 수렴한다. 학습이 잘 된다는 것은 좋은 가중치를 얻어서 원하는 출력에 점점 가까운 값을 얻는 것이라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $$sigmoid : \\sigma(z) = \\frac{{1}}{{1}+{exp(-z)}}$$     $$ReLU : (z) = max(0, z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 신경망 초기에는 sigmoid가 주로 사용되었지만, 은닉층을 다수 사용하는 딥러닝 시대가 되면서 ReLU가 더 많이 사용되고 있습니다. 딥러닝에서 오류를 역전파(backpropagation)할 때 sigmoid함수가 값을 점점 작아지게 하는 문제를 toronto대학의  Vinod Nair와 Geoffrey Hinton교수가 지적하며, ReLU를 대안으로 제시한 2010년 논문에 이에 대한 자세한 내용이 나와 있다. 그래프를 그려보면 sigmoid는 출력값을 0~1 사이로만 제한하게 되지만, ReLU는 양수를 그대로 반환하기 때문에 값의 왜곡이 적어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid함수를 활성화함수로 해서(출력을 0~1사이로 제한) AND, OR, XOR연산을 실행해보자\n",
    "# 먼저 Sigmoid함수를 구현한다\n",
    "\n",
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37668917897651105\n"
     ]
    }
   ],
   "source": [
    "# 입력이 1일 때 기대출력이 0이 되는 뉴런을 만들어 보자\n",
    "# error = y - output --> error가 0에 가까워지게 하는 출력으로 기댓값에 가까운 값을 얻는 것이다.\n",
    "\n",
    "x = 1\n",
    "y = 0\n",
    "w = tf.random.normal([1], 0, 1)\n",
    "output = sigmoid(w * x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시행횟수 :  99,  error : -0.003251,  output : 0.003251\n",
      "시행횟수 : 199,  error : -0.003149,  output : 0.003149\n",
      "시행횟수 : 299,  error : -0.003053,  output : 0.003053\n",
      "시행횟수 : 399,  error : -0.002963,  output : 0.002963\n",
      "시행횟수 : 499,  error : -0.002878,  output : 0.002878\n",
      "시행횟수 : 599,  error : -0.002798,  output : 0.002798\n",
      "시행횟수 : 699,  error : -0.002722,  output : 0.002722\n",
      "시행횟수 : 799,  error : -0.002650,  output : 0.002650\n",
      "시행횟수 : 899,  error : -0.002582,  output : 0.002582\n",
      "시행횟수 : 999,  error : -0.002517,  output : 0.002517\n"
     ]
    }
   ],
   "source": [
    "# 뉴런이란 결국 w값이다. 이제 이 w를 변화시켜야 하는데, 경사하강법(Gradient Descent)을 이용한다.\n",
    "# w = w + x * a * error, where a = learning rate(학습률) = 0.1\n",
    "\n",
    "for i in range(1000):\n",
    "    output = sigmoid(x * w)\n",
    "    error = y - output\n",
    "    w = w + x * 0.1 * error\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(f\"시행횟수 : {i:3d},  error : {error:8f},  output : {output:8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b18c970f1c267b2ae7de1f51c7b9ea29dc67f6c9f87d5b0ef47c4534e0830b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
