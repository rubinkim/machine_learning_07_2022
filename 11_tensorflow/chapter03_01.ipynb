{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-55.28862, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_sum(tf.random.normal([1000,1000])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow 2.10.0 시작하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "# Hello world \n",
    "# 텐서플로는  C++와 python을 기반으로 한다. Google Colab도 python을 기본적으로 지원하고 있다.\n",
    "print(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 텐서플로는 GitHub에 공개된 open source library로서 약 75,000개의 fork가 만들어지고 2,100여명의 코드 기여자가 있으며, 거의 매 주마다 마이너 버전이 업데이트된다. Library의 version정보가 다르면 잘 돌아가야 할 코드에서 에러가 발생할 수도 있다. 설치한 텐서플로의 버전을 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### perceptron의 한계를 지적하는데 사용됐던 AND, OR, XOR 연산을 할 수 있는 신경망 네트워크를 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.5670587], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# [0,1] 구간의 uniform distribution에서 shape=(1,)인 난수 생성\n",
    "rand = tf.random.uniform([1], 0, 1)\n",
    "print(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.38932252 0.13187706 0.93992686 0.31947803], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# [0,1] 구간의 uniform distribution에서 shape=(4,)인 난수 생성\n",
    "rand = tf.random.uniform(shape=[4], minval=0, maxval=1)\n",
    "print(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.6506895  -0.53620243]\n",
      " [-0.18717213  0.0853856 ]\n",
      " [-0.07208761  0.6546105 ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 평균이 0, 표준편차가 1 인 정규분포에서 shape=(3,2)인 난수 행렬 생성\n",
    "rand = tf.random.normal(shape=[3,2], mean=0, stddev=1)\n",
    "print(rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 신경망의 가장 기본적인 구성요소인 뉴런을 만들어 보자.<br>뉴런이 여러개 모여 layer를 구성한 후 이 layer들이 모여서 신경망을 구성한다.<br>뉴런은 입력, 가중치, 활성화함수, 출력으로 구성된다.<br>가장 간단한 형태의 뉴런은 입력에 가중치를 곱한 뒤(행렬의 곱셈) 활성화함수를 취하면 출력을 얻을 수 있다.<br>뉴런에서 학습할 때 변하는 것은 가중치이다. 가중치는 처음에는 초기화를 통해 랜덤한 값을 넣고, 학습 과정에서 점차 일정한 값으로 수렴한다. 학습이 잘 된다는 것은 좋은 가중치를 얻어서 원하는 출력에 점점 가까운 값을 얻는 것이라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $$sigmoid : \\sigma(z) = \\frac{{1}}{{1}+{exp(-z)}}$$     $$ReLU : (z) = max(0, z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 신경망 초기에는 sigmoid가 주로 사용되었지만, 은닉층을 다수 사용하는 딥러닝 시대가 되면서 ReLU가 더 많이 사용되고 있습니다. 딥러닝에서 오류를 역전파(backpropagation)할 때 sigmoid함수가 값을 점점 작아지게 하는 문제를 toronto대학의  Vinod Nair와 Geoffrey Hinton교수가 지적하며, ReLU를 대안으로 제시한 2010년 논문에 이에 대한 자세한 내용이 나와 있다. 그래프를 그려보면 sigmoid는 출력값을 0~1 사이로만 제한하게 되지만, ReLU는 양수를 그대로 반환하기 때문에 값의 왜곡이 적어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid함수를 활성화함수로 해서(출력을 0~1사이로 제한) AND, OR, XOR연산을 실행해보자\n",
    "# 먼저 Sigmoid함수를 구현한다\n",
    "\n",
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20798553599941239\n"
     ]
    }
   ],
   "source": [
    "# 입력이 1일 때 기대출력이 0이 되는 뉴런을 만들어 보자\n",
    "# error = y - output --> error가 0에 가까워지게 하는 출력으로 기댓값에 가까운 값을 얻는 것이다.\n",
    "\n",
    "x = 1\n",
    "y = 0\n",
    "w = tf.random.normal([1], 0, 1)\n",
    "output = sigmoid(w * x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시행횟수 :  99,  error : -0.073713,  output : 0.073713\n",
      "시행횟수 : 199,  error : -0.043419,  output : 0.043419\n",
      "시행횟수 : 299,  error : -0.030595,  output : 0.030595\n",
      "시행횟수 : 399,  error : -0.023568,  output : 0.023568\n",
      "시행횟수 : 499,  error : -0.019147,  output : 0.019147\n",
      "시행횟수 : 599,  error : -0.016113,  output : 0.016113\n",
      "시행횟수 : 699,  error : -0.013905,  output : 0.013905\n",
      "시행횟수 : 799,  error : -0.012226,  output : 0.012226\n",
      "시행횟수 : 899,  error : -0.010907,  output : 0.010907\n",
      "시행횟수 : 999,  error : -0.009844,  output : 0.009844\n"
     ]
    }
   ],
   "source": [
    "# 뉴런이란 결국 w값이다. 이제 이 w를 변화시켜야 하는데, 경사하강법(Gradient Descent)을 이용한다.\n",
    "# w = w + x * a * error, where a = learning rate(학습률) --> 경사하강법의 가중치 update식\n",
    "x = 1\n",
    "y = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    output = sigmoid(x * w)\n",
    "    error = y - output\n",
    "    w = w + x * 0.1 * error  ## a = 0.1\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(f\"시행횟수 : {i:3d},  error : {error:8f},  output : {output:8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시행횟수 :  99,  error : 0.132009,  output : 0.867991\n",
      "시행횟수 : 199,  error : 0.059730,  output : 0.940270\n",
      "시행횟수 : 299,  error : 0.038037,  output : 0.961963\n",
      "시행횟수 : 399,  error : 0.027792,  output : 0.972208\n",
      "시행횟수 : 499,  error : 0.021859,  output : 0.978141\n",
      "시행횟수 : 599,  error : 0.017999,  output : 0.982001\n",
      "시행횟수 : 699,  error : 0.015290,  output : 0.984710\n",
      "시행횟수 : 799,  error : 0.013286,  output : 0.986714\n",
      "시행횟수 : 899,  error : 0.011744,  output : 0.988256\n",
      "시행횟수 : 999,  error : 0.010522,  output : 0.989478\n"
     ]
    }
   ],
   "source": [
    "# 만약 x=0로 하고 실행하면 error값은 변하지 않고 이에 따라 w, output값들 역시 변하지 않게 된다.\n",
    "# 이런 경우를 방지하기 위해 bias(편향)을 뉴런에 넣어준다. 편향값으로 보통 1을 사용한다.\n",
    "x = 0\n",
    "y = 1\n",
    "w = tf.random.normal([1], 0, 1)\n",
    "b = tf.random.normal([1], 0, 1)\n",
    "\n",
    "for i in range(1000):\n",
    "    output = sigmoid(x * w + 1 * b)\n",
    "    error = y - output\n",
    "    w = w + x * 0.1 * error\n",
    "    b = b + 1 * 0.1 * error\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(f\"시행횟수 : {i:3d},  error : {error:8f},  output : {output:8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow's version : 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "print(f\"tensorflow's version : {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시행횟수 :  99,  error : 0.106724,  output : 0.893276\n",
      "시행횟수 : 199,  error : 0.053596,  output : 0.946404\n",
      "시행횟수 : 299,  error : 0.035404,  output : 0.964596\n",
      "시행횟수 : 399,  error : 0.026347,  output : 0.973653\n",
      "시행횟수 : 499,  error : 0.020950,  output : 0.979050\n",
      "시행횟수 : 599,  error : 0.017376,  output : 0.982624\n",
      "시행횟수 : 699,  error : 0.014837,  output : 0.985163\n",
      "시행횟수 : 799,  error : 0.012942,  output : 0.987058\n",
      "시행횟수 : 899,  error : 0.011474,  output : 0.988526\n",
      "시행횟수 : 999,  error : 0.010304,  output : 0.989696\n"
     ]
    }
   ],
   "source": [
    "# 만약 x=0로 하고 실행하면 error값은 변하지 않고 이에 따라 w, output값들 역시 변하지 않게 된다.\n",
    "# 이런 경우를 방지하기 위해 bias(편향)을 뉴런에 넣어준다. 편향값으로 보통 1을 사용한다.\n",
    "x = 0\n",
    "y = 1\n",
    "w = tf.random.normal([1], 0, 1)\n",
    "b = tf.random.normal([1], 0, 1)\n",
    "\n",
    "for i in range(1000):\n",
    "    output = sigmoid(x * w + 1 * b)\n",
    "    error = y - output\n",
    "    w = w + x * 0.1 * error\n",
    "    b = b + 1 * 0.1 * error\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(f\"시행횟수 : {i:3d},  error : {error:8f},  output : {output:8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시행횟수 :  999,  error : -0.005056,  output : 0.005056\n",
      "시행횟수 : 1999,  error : -0.002518,  output : 0.002518\n",
      "시행횟수 : 2999,  error : -0.001676,  output : 0.001676\n",
      "시행횟수 : 3999,  error : -0.001255,  output : 0.001255\n",
      "시행횟수 : 4999,  error : -0.001004,  output : 0.001004\n",
      "시행횟수 : 5999,  error : -0.000836,  output : 0.000836\n",
      "시행횟수 : 6999,  error : -0.000716,  output : 0.000716\n",
      "시행횟수 : 7999,  error : -0.000627,  output : 0.000627\n",
      "시행횟수 : 8999,  error : -0.000557,  output : 0.000557\n",
      "시행횟수 : 9999,  error : -0.000501,  output : 0.000501\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "y = 0\n",
    "\n",
    "w = tf.random.normal([1], 0, 1)\n",
    "b = tf.random.normal([1], 0, 1)\n",
    "\n",
    "for i in range(10000):\n",
    "    output = new_sigmoid(x * w + 1 * b)\n",
    "    error = y - output\n",
    "    w = w + x * error * 0.1         # eLoss/ew = x * error * learning_rate\n",
    "    b = b + 1 * error * 0.1         # eLoss/eb = 1 * error * learning_rate\n",
    "    \n",
    "    if i % 1000 == 999:\n",
    "        print(f\"시행횟수 : {i:4d},  error : {error:8f},  output : {output:8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시행횟수 :  999,  error : [-4.7683716e-07],  output : [4.7683716e-07]\n",
      "시행횟수 : 1999,  error : [-4.7683716e-07],  output : [4.7683716e-07]\n",
      "시행횟수 : 2999,  error : [-4.7683716e-07],  output : [4.7683716e-07]\n",
      "시행횟수 : 3999,  error : [-4.7683716e-07],  output : [4.7683716e-07]\n",
      "시행횟수 : 4999,  error : [-4.7683716e-07],  output : [4.7683716e-07]\n",
      "시행횟수 : 5999,  error : [-4.7683716e-07],  output : [4.7683716e-07]\n",
      "시행횟수 : 6999,  error : [-4.7683716e-07],  output : [4.7683716e-07]\n",
      "시행횟수 : 7999,  error : [-4.7683716e-07],  output : [4.7683716e-07]\n",
      "시행횟수 : 8999,  error : [-4.7683716e-07],  output : [4.7683716e-07]\n",
      "시행횟수 : 9999,  error : [-4.7683716e-07],  output : [4.7683716e-07]\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "y = 0\n",
    "\n",
    "w = tf.random.normal([1], 0, 1)\n",
    "b = tf.random.normal([1], 0, 1)\n",
    "\n",
    "for i in range(10000):\n",
    "    output = ReLU(x * w + 1 * b)\n",
    "    error = y - output\n",
    "    w = w + x * error * 0.1         # eLoss/ew = x * error * learning_rate\n",
    "    b = b + 1 * error * 0.1         # eLoss/eb = 1 * error * learning_rate\n",
    "    \n",
    "    if i % 1000 == 999:\n",
    "        print(f\"시행횟수 : {i:4d},  error : {error},  output : {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 첫번째 신경망 네트워크 : AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시행횟수 :   199,   오차합 : -0.059728\n",
      "시행횟수 :   399,   오차합 : -0.034430\n",
      "시행횟수 :   599,   오차합 : -0.024037\n",
      "시행횟수 :   799,   오차합 : -0.018405\n",
      "시행횟수 :   999,   오차합 : -0.014885\n",
      "시행횟수 :  1199,   오차합 : -0.012484\n",
      "시행횟수 :  1399,   오차합 : -0.010742\n",
      "시행횟수 :  1599,   오차합 : -0.009423\n",
      "시행횟수 :  1799,   오차합 : -0.008391\n",
      "시행횟수 :  1999,   오차합 : -0.007561\n"
     ]
    }
   ],
   "source": [
    "# AND연산을 수행하는 뉴런을 만들어보자.\n",
    "# AND연산은 두개의 입력값(True/False)중 모두 True여야만 출력값이 True이고 나머지 경우는 모두 False가 되는 연산이다.\n",
    "# x = [[1,1], [1,0], [0,1], [0,0]] ----> y = [1, 0, 0, 0]\n",
    "# 입력값의 차원이 2이므로 가중치 역시 2차원이어야 한다.\n",
    "# 절편(bias)은 1차원이고 값은 1로 한다.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[1,1],[1,0],[0,1],[0,0]])\n",
    "y = np.array([[1],[0],[0],[0]])\n",
    "w = tf.random.normal([2], 0, 1)\n",
    "b = tf.random.normal([1], 0, 1)\n",
    "b_x = 1\n",
    "\n",
    "for i in range(2000):\n",
    "    error_sum = 0\n",
    "    for j in range(4):\n",
    "        output = sigmoid(np.sum(x[j] * w + b_x * b))\n",
    "        error = y[j][0] - output\n",
    "        w = w + x[j] * 0.1 * error\n",
    "        b = b + b_x * 0.1 * error\n",
    "        error_sum += error\n",
    "                \n",
    "    if i % 200 == 199:\n",
    "        print(f\"시행횟수 :  {i:4d},   오차합 : {error_sum:3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습시킨 가중치 : [7.5791, 7.584],   편차 : [-5.8036]\n",
      "X : [1 1],   Y : 1,    Output : 0.97224\n",
      "X : [1 0],   Y : 0,    Output : 0.0175\n",
      "X : [0 1],   Y : 0,    Output : 0.01758\n",
      "X : [0 0],   Y : 0,    Output : 1e-05\n"
     ]
    }
   ],
   "source": [
    "# 이렇게 학습시킨 네트워크가 정상적으로 작동하는지 평가해보자.\n",
    "print(f\"학습시킨 가중치 : {[np.round(x, 4) for x in w]},   편차 : {np.round(b, 4)}\")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"X : {x[i]},   Y : {y[i][0]},    Output : {np.round(sigmoid(np.sum(x[i] * w + b_x * b)), 5)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b18c970f1c267b2ae7de1f51c7b9ea29dc67f6c9f87d5b0ef47c4534e0830b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
